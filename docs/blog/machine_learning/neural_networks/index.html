<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=apple-mobile-web-app-capable content="yes"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#50bf9e"><meta name=generator content="Hugo 0.74.3"><meta name=robots content="index, follow"><title>An Attempt to Copy Human Learning with the Help of Artificial Neural Networks | swissbib data blog</title><link rel=canonical href=https://swissbib.github.io/blog/machine_learning/neural_networks/><meta name=description content="using neural networks"><meta name=author content="project swissbib"><link rel=manifest href=/manifest.json><meta property="og:title" content="An Attempt to Copy Human Learning with the Help of Artificial Neural Networks"><meta property="og:site_name" content="swissbib data blog"><meta property="og:description" content="using neural networks"><meta property="og:type" content="article"><meta property="og:url" content="https://swissbib.github.io/blog/machine_learning/neural_networks/"><meta property="article:published_time" content="2020-11-11T20:00:00+02:00"><meta property="article:modified_time" content="2020-11-11T20:00:00+02:00"><meta property="article:section" content="blog"><meta property="article:tag" content="neural networks"><meta property="article:tag" content="english"><meta property="article:tag" content="2020"><meta name=twitter:card content="summary"><meta name=twitter:title content="An Attempt to Copy Human Learning with the Help of Artificial Neural Networks"><meta name=twitter:description content="using neural networks"><meta name=twitter:site content="@https://twitter.com/swissbib"><meta name=twitter:creator content="@https://twitter.com/swissbib"><meta name=news_keywords content="neural networks,english,2020"><script type=application/ld+json>{"headline":"An Attempt to Copy Human Learning with the Help of Artificial Neural Networks","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/image/logo.min.svg"},"name":"project swissbib"},"author":{"@type":"Person","name":"project swissbib"},"description":"using neural networks","name":"swissbib data blog","@type":"BlogPostingblog","wordCount":1595,"mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/swissbib.github.io\/blog\/machine_learning\/neural_networks\/"},"@context":"http://schema.org","dateCreated":"2020-11-11T20:00:00+02:00","dateModified":"2020-11-11T20:00:00+02:00","url":"https://swissbib.github.io/blog/machine_learning/neural_networks/"}</script><link rel=preload href=/index.json as=fetch crossorigin importance=low><link rel=stylesheet href=/css/index.min.d038f1a09fadf7102851d162968e2191579591b1a81db498d110a28635df30bd382fcb6f9e1c9be70b999bcb7fc7e2c5f671d7f5dd9e186f91c4de01dfd9ecd5.css media=none onload="if(media!='all')media='all'"><noscript><link rel=stylesheet href=/css/index.min.d038f1a09fadf7102851d162968e2191579591b1a81db498d110a28635df30bd382fcb6f9e1c9be70b999bcb7fc7e2c5f671d7f5dd9e186f91c4de01dfd9ecd5.css media=screen></noscript><style>html{opacity:0}</style><script type=text/javascript src=/index.min.js defer></script><script>if(window.localStorage.color){document.documentElement.style.setProperty("--theme-color",window.localStorage.color);}
if(window.localStorage.dark==="true"){document.documentElement.classList.add("dark");}</script></head><body><a href=#top class=scroll-up role=button><span class=sr-only>Scroll to top</span><svg width="30" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M16 6.59375l-.71875.6875-12.5 12.5 1.4375 1.4375L16 9.4375 27.78125 21.21875l1.4375-1.4375-12.5-12.5L16 6.59375z"/></svg></a><header id=header><input class=hamburger role=button type=checkbox aria-label="Menu Button">
<a class=icon aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 25 28"><g class="svg-menu-toggle"><path class="bar" d="M20.945 8.75c0 .69-.5 1.25-1.117 1.25H3.141c-.617.0-1.118-.56-1.118-1.25s.5-1.25 1.118-1.25h16.688C20.445 7.5 20.945 8.06 20.945 8.75z"/><path class="bar" d="M20.923 15c0 .689-.501 1.25-1.118 1.25H3.118C2.5 16.25 2 15.689 2 15s.5-1.25 1.118-1.25h16.687C20.422 13.75 20.923 14.311 20.923 15z"/><path class="bar" d="M20.969 21.25c0 .689-.5 1.25-1.117 1.25H3.164c-.617.0-1.118-.561-1.118-1.25s.5-1.25 1.118-1.25h16.688C20.469 20 20.969 20.561 20.969 21.25z"/></g></svg></a><a role=button class=logo href=https://swissbib.github.io/index.html><svg xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:cc="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" enable-background="new 0 0 500 150" viewBox="0 0 388.97198 121.52146" height="121.52146" width="388.97198" id="Ebene_1"><defs id="defs77"/><g transform="translate(-59.79,-4.3615414)" id="g3"><g id="g5"><defs id="defs7"><polygon points="-22,325 -22,-85 -22,-85 573.273,-85 573.273,325" id="SVGID_1_"/></defs><clipPath id="SVGID_2_"><use height="100%" width="100%" y="0" x="0" style="overflow:visible" id="use11" overflow="visible" xlink:href="#SVGID_1_"/></clipPath><path style="fill:#fff" id="path15" d="m310.63 60.843c0 3.024-2.45 5.478-5.473 5.478-3.024.0-5.475-2.453-5.475-5.478.0-3.022 2.45-5.473 5.475-5.473 3.023.0 5.473 2.45 5.473 5.473z" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path17" d="m171.77 60.67c0 3.023-2.453 5.477-5.477 5.477-3.023.0-5.473-2.453-5.473-5.477s2.45-5.473 5.473-5.473c3.024.0 5.477 2.449 5.477 5.473z" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path19" d="m60.563 115.953c3.757 2.217 7.807 3.184 11.567 3.184 4.337.0 8-1.834 8-6.75.0-9.927-19.857-8.289-19.857-23.229.0-9.544 8.097-13.497 15.71-13.497 3.377.0 7.23.482 10.22 1.736l-.58 7.23C82.83 83.18 79.747 82.41 76.37 82.41c-3.567.0-7.517 1.637-7.517 6.263.0 10.317 19.857 8.004 19.857 22.847.0 9.737-7.423 14.363-16.197 14.363-4.53.0-8.77-.673-12.723-2.12l.773-7.81z" clip-path="url(#SVGID_2_)"/><polygon style="fill:#fff" id="polygon21" points="110.3,116.63 119.17,76.623 128.81,76.623 137.677,116.63 146.35,76.623 154.543,76.623 142.207,124.823 132.567,124.823 123.797,85.396 114.83,124.823 105.19,124.823 105.19,124.823 92.853,76.623 101.72,76.623" clip-path="url(#SVGID_2_)"/><polygon style="fill:#fff" id="polygon23" points="170.353,124.823 162.063,124.823 162.063,124.823 162.063,76.623 170.353,76.623" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path25" d="m180.953 115.953c3.76 2.217 7.81 3.184 11.567 3.184 4.34.0 8-1.834 8-6.75.0-9.927-19.853-8.289-19.853-23.229.0-9.544 8.096-13.497 15.71-13.497 3.373.0 7.23.482 10.22 1.736l-.58 7.23c-2.797-1.447-5.88-2.217-9.254-2.217-3.566.0-7.52 1.637-7.52 6.263.0 10.317 19.857 8.004 19.857 22.847.0 9.737-7.42 14.363-16.193 14.363-4.53.0-8.77-.673-12.724-2.12l.77-7.81z" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path27" d="m216.33 115.953c3.76 2.217 7.807 3.184 11.567 3.184 4.335.0 8-1.834 8-6.75.0-9.927-19.856-8.289-19.856-23.229.0-9.544 8.097-13.497 15.713-13.497 3.374.0 7.229.482 10.218 1.736l-.578 7.23c-2.795-1.447-5.879-2.217-9.252-2.217-3.57.0-7.521 1.637-7.521 6.263.0 10.317 19.856 8.004 19.856 22.847.0 9.737-7.424 14.363-16.193 14.363-4.533.0-8.773-.673-12.723-2.12l.769-7.81z" clip-path="url(#SVGID_2_)"/><polygon style="fill:#fff" id="polygon29" points="309.25,124.823 300.96,124.823 300.96,124.823 300.96,76.623 309.25,76.623" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path31" d="m272.141 119.137c-8.457.0-9.928-12.27-9.928-18.607.0-7.036 1.54-18.119 9.928-18.119 8.482.0 9.542 11.857 9.542 18.119.0 6.074-1.35 18.607-9.542 18.607zm2.312-43.477c-5.88.0-9.641 3.18-12.051 7.713v-28.051h-8.285v46.697c0 10.061 2.482 23.863 17.976 23.863 15.527.0 18.46-15.326 18.46-25.354.0-10.408-2.991-24.868-16.1-24.868z" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path33" d="m340.29 119.137c-8.457.0-9.93-12.27-9.93-18.607.0-7.036 1.543-18.119 9.93-18.119 8.479.0 9.54 11.857 9.54 18.119.0 6.074-1.347 18.607-9.54 18.607zm2.313-43.477c-5.88.0-9.64 3.18-12.05 7.713v-28.051h-8.29v46.697c0 10.061 2.484 23.863 17.978 23.863 15.529.0 18.46-15.326 18.46-25.354C358.7 90.12 355.713 75.66 342.603 75.66z" clip-path="url(#SVGID_2_)"/><path style="fill:#fdeb18" id="path35" d="m367.88 36.62c.003-.07.01-.138.013-.2-.003.062-.01.13-.013.2z" clip-path="url(#SVGID_2_)"/><path style="fill:#fdeb18" id="path37" d="m368.03 35.247c.029-.207.05-.41.08-.614-.03.204-.053.407-.08.614z" clip-path="url(#SVGID_2_)"/><path style="fill:#fdeb18" id="path39" d="m389.997 72.543c-.007-.003-.017-.006-.024-.01.007.004.017.007.024.01z" clip-path="url(#SVGID_2_)"/><path style="fill:#fdeb18" id="path41" d="m403.337 75.05c-.05.003-.1.003-.15.003.05.0.1.0.15-.003z" clip-path="url(#SVGID_2_)"/><path style="fill:#fdeb18" id="path43" d="m406.727 74.87c-.124.01-.243.017-.366.03.122-.009.242-.02.366-.03z" clip-path="url(#SVGID_2_)"/><path style="fill:#fdeb18" id="path45" d="m405.06 75c-.093.003-.189.007-.286.01.096-.003.193-.007.286-.01z" clip-path="url(#SVGID_2_)"/><path style="fill:#fdeb18" id="path47" d="m388.443 71.883c-.014-.006-.03-.016-.047-.023.017.007.034.017.047.023z" clip-path="url(#SVGID_2_)"/><path style="fill:#fdeb18" id="path49" d="m397.327 20.85c1.746.42 3.413.998 5.006 1.693 4.729-3.141 10.407-4.973 16.51-4.973 5.217.0 10.117 1.337 14.388 3.683-.591-.963-1.23-1.905-1.921-2.826-11.757-15.587-33.917-18.69-49.51-6.94-7.566 5.707-12.18 13.871-13.59 22.541.32-1.944.797-3.857 1.43-5.715 7.12-6.79 17.42-9.946 27.687-7.463z" clip-path="url(#SVGID_2_)"/><path style="fill:#2a245a" id="path51" d="m433.23 21.253c9.412 15.345 5.803 35.617-8.86 46.677-5.323 4.01-11.417 6.291-17.597 6.93l-.004.004c3.697 1.63 7.777 2.547 12.073 2.547 16.521.0 29.92-13.4 29.92-29.92C448.763 36.18 442.487 26.34 433.23 21.253z" clip-path="url(#SVGID_2_)"/><path style="fill:#cbba9f" id="path53" d="m402.333 22.543c13.03 5.71 20.507 20.104 17.044 34.417-1.85 7.653-6.51 13.88-12.604 17.899 6.18-.639 12.273-2.92 17.597-6.93 14.663-11.06 18.272-31.332 8.86-46.677-4.271-2.346-9.171-3.683-14.388-3.683-6.102.001-11.78 1.833-16.509 4.974z" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path55" d="m406.36 74.9c-.431.039-.867.072-1.301.1.434-.027.871-.061 1.301-.1z" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path57" d="m404.773 75.01c-.477.023-.956.04-1.437.04.481-.003.957-.017 1.437-.04z" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path59" d="m406.727 74.87c.014-.003.03-.003.043-.007-.013.0-.03.004-.043.007z" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path61" d="m403.187 75.053c-4.504.008-8.986-.84-13.189-2.51 4.165 1.657 8.642 2.523 13.189 2.51z" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path63" d="m368.11 34.633c.029-.203.066-.402.1-.605-.033.202-.07.402-.1.605z" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path65" d="m367.893 36.42c.04-.393.088-.783.138-1.173-.051.39-.098.78-.138 1.173z" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path67" d="m374.87 60.99c-5.483-7.27-7.73-15.967-6.99-24.37-.517 5.67.323 11.543 2.75 17.144 3.62 8.356 10.083 14.596 17.767 18.096-5.177-2.36-9.86-6.006-13.527-10.87z" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path69" d="m389.973 72.533c-.513-.203-1.022-.424-1.529-.65.506.226 1.016.447 1.529.65z" clip-path="url(#SVGID_2_)"/><polygon style="fill:#fff" id="polygon71" points="406.77,74.863 406.773,74.859 406.77,74.863" clip-path="url(#SVGID_2_)"/><path style="fill:#fff" id="path73" d="m402.333 22.543c-1.593-.695-3.26-1.273-5.006-1.693-10.268-2.482-20.567.674-27.688 7.463-.633 1.857-1.109 3.771-1.43 5.715-.033.203-.07.402-.1.605-.03.204-.051.407-.08.614-.05.39-.098.78-.138 1.173-.003.062-.01.13-.013.2-.74 8.403 1.507 17.101 6.99 24.37 3.667 4.863 8.35 8.51 13.526 10.869.017.008.033.018.047.023.507.227 1.017.447 1.529.65.008.004.018.007.024.01 4.203 1.67 8.686 2.518 13.189 2.51.051.0.101.0.15-.003.48.0.96-.017 1.437-.04.097-.003.193-.007.286-.01.434-.027.87-.061 1.301-.1.123-.014.242-.021.366-.03.014-.003.03-.007.043-.007l.004-.004c6.094-4.02 10.754-10.246 12.604-17.899 3.466-14.313-4.011-28.706-17.041-34.416z" clip-path="url(#SVGID_2_)"/></g></g></svg><span class=sr-only>Logo</span></a><ul role=menubar class=top-menu><li role=menuitem class=top-menu-item><a href=/blog/>BLOG</a><ul class=sub-menu role=menu><li><a href=/blog/machine_learning/>MACHINE LEARNING</a></li></ul></li><li role=menuitem class=top-menu-item><a href=/categories>CATEGORIES</a></li><li role=menuitem class=top-menu-item><a href=/tags>TAGS</a></li></ul><div id=searchbox><label class=sr-only for=search>Search</label>
<input id=search type=text placeholder=Search spellcheck=true><svg width="20" class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path fill="currentcolor" d="M19 3C13.488997 3 9 7.4889972 9 13 9 15.39499 9.8389508 17.588106 11.25 19.3125L3.28125 27.28125l1.4375 1.4375L12.6875 20.75C14.411894 22.161049 16.60501 23 19 23 24.511003 23 29 18.511003 29 13 29 7.4889972 24.511003 3 19 3zm0 2C23.430123 5 27 8.5698774 27 13 27 17.430123 23.430123 21 19 21 14.569877 21 11 17.430123 11 13 11 8.5698774 14.569877 5 19 5z"/></svg><section class=results></section></div><div id=scroll-indicator></div></header><section class="meta content with-background"><article><h1 class=title>An Attempt to Copy Human Learning with the Help of Artificial Neural Networks</h1><p>Besides the availability of data, an additional reason why Machine Learning has become a popular topic has to do with publicly available code libraries that simplify the implementation of machines. One such library is <a href="https://www.tensorflow.org/?hl=en" target=_blank>TensorFlow</a>, an open-source platform written by <a href=https://en.wikipedia.org/wiki/Google_Brain target=_blank>Google Brain</a>. TensorFlow can be used for implementing Neural Networks. Since the end of 2019 a library called <a href=https://keras.io/ target=_blank>keras</a> has been integrated on top of TensorFlow. This library makes the code implementation of Neural Networks even easier. The swissbib project at hand is as far as known the first implementation of a Neural Network for deduplication based on keras. This blog article explains Neural Networks and compares the results of its implementation with the results of the preceding blog articles.</p><p>The strength of a conventional computer is its ability to execute arithmetic and logical operations extremely fast. This is due to its hardware, consisting of semiconductor transistors as building blocks. A conventional computer needs a sequence of precise and syntactically correct commands for executing a task. Any error or inaccuracy in such a sequence of commands will lead to failure in its execution. For this lack of fault tolerance, conventional computers have no ability to learn from data. The human brain on the other hand, is strong in learning and transferring learnt patterns to new and unknown conditions. This ability helps in handling errors and inaccuracies, it is even able to predict results under unknown situations. It is a strength of solving semantic tasks. Its abilities are based on its building blocks, the <a href=https://en.wikipedia.org/wiki/Neuron target=_blank>electrically excitable nerve cells</a> called neurons together with their network structure. A neuron is a biological cell with the ability to propagate an electrical signal to its neighbouring cells. Artificial Neural Networks are the attempt to mimic the functionality of a system of biological nerves with the goal of making use of the strengths of such systems.</p><p>With regard to the specific swissbib problem at hand, the <a href=/blog/machine_learning/support_vector_machine/ target=_blank>second blog article</a> talks about a boundary, separating the records of pairs of uniques from the records of pairs of duplicates. As soon as the position of this boundary is determined, a conventional computer can decide very quickly to which side a new and unknown record is to be associated. It is the learning process on where to place the best boundary in the training data set, that is best resolved with the help of statistical learning like e.g. an artificial Neural Network. The building blocks of an artificial Neural Network are a try to copy the mode of operation of the building blocks of a biological neural network. The signal processing of a biological nerve cell is described with the help of a physical model which can be read <a href=https://en.wikipedia.org/wiki/Hodgkin-Huxley_model target=_blank>elsewhere</a>. Besides the mode of operation of its building blocks, the network topology is fundamental for the abilities of a Neural Network. In Computer Science, there are several known topologies used for the network structure of neurons. Certain topologies have been invented, developed, and used for specific kinds of problems to solve. A nice animation is shown in the youtube video below.<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/3JQ3hYko51Y style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></p><p>As for the topology of an artificial Neural Network, one set of neurons forms an input layer for the data records to be processed, see figure 1. Another set of neurons forms an output layer for classifying the input record to either a record of uniques or a record of duplicates for the swissbib data. In between these boundary layers may be found the so-called hidden layers. Each artificial neuron of one specific layer is connected to each neuron of its neighbouring layers. Each connection between two neurons has a weight which multiplies the output signal of a source neuron to generate one of the input signals of a target neuron. The training process adjusts these connections continuously, together with a threshold value that determines the output of a neuron as a function of its input. This is done by comparing the answer of the network with its expected result after applying the signal of a record to the input layer. The training process can be terminated when any further adjustments do not significantly improve the output compared to the expected results anymore. The specific values of the weights together with the thresholds of the artificial neurons are the result of the training process.</p><h2 id=a-hrefimagemlneural_networkpng-target_blankimg-style-width-100-height-100-srcimagemlneural_networkpnga><a href=/image/ml/neural_network.png target=_blank><img style=width:100%;height:100% src=/image/ml/neural_network.png></a></h2><h2 id=figure-1-neural-network>figure 1, neural network</h2><br><br><p>For the swissbib project at hand, a Neural Network with two hidden layers between the input and the output layers has been implemented, see figure 1. The input layer consists of twenty neurons, one for each distinct similarity of a feature record, described in the <a href=/blog/machine_learning/ensemblemethods target=_blank>first blog article</a>. The output layer holds two neurons, one that fires a signal if the input record is identified as a record of pairs of duplicates and another one that fires a signal if the input record is identified as a record of pairs of uniques. The first hidden layer that is connected to the input layer as its input and to the second layer as its output layer, consists of 40 neurons in the best topology found for the project. The second hidden layer that is connected to the first hidden layer as its input and to the output layer as its output layer, consists of 75 neurons in the topology of the Neural Network found with the best performance values. In terms of accuracy, a value of 99.93% has shown to be an upper limit that no Neural Network tested could exceed. This maximum accuracy is lower than the best accuracy values of the best Random Forests model of the <a href=/blog/machine_learning/ensemblemethods/ target=_blank>first blog article</a> but it is higher than the best accuracy values of the Support Vector Machine approach described in the <a href=/blog/machine_learning/support_vector_machine/ target=_blank>second blog article</a>. This maximum accuracy for the Neural Network means a number of wrong predictions of close to 30 on a total of 51'886 validation records.</p><p>It is remarkable to point out the large number of artificial neurons of the hidden layers used for the project at hand. This large number of neurons may hint to a strong need of interactions between the features for the Neural Network to be a successful classifier. Discussing this observation with the swissbib project team, the team members explain it with some analogies out of the conventional processing of deduplication. As an example, some features may be very dominant in the sense that if they show a large similarity value, this is a strong indicator for a record of pairs of duplicates. On the other hand, records with titles consisting of only few words may be identified as duplicates if and only if other attributes show a large similarity value. This becomes even more relevant, and the need for sophistication may raise, in case of empty attributes in one of the records of the pair. For these various situations, very different path options for the signal through the network from input to output layer may be helpful.</p><p>The project tries topologies with one, two, and three hidden layers and observes that a two-layer network generates the best results in relation to its training duration. One property found for the Neural Network eventually related to the high number of neurons is its low learning rate value of ideally 0.002, which makes the learning process very slow but gives more stability to the network during its learning phase. This is expressed in a slowly converging learning curve during the training process, see figure 2.</p><h2 id=a-hrefimagemlconverging_learningpng-target_blankimg-style-width-100-height-100-srcimagemlconverging_learningpnga><a href=/image/ml/converging_learning.png target=_blank><img style=width:100%;height:100% src=/image/ml/converging_learning.png></a></h2><h2 id=figure-2-converging-learning-curve>figure 2, converging learning curve</h2><br><br><p>Comparing the accuracy of the Neural Network solution described here with the <a href=/blog/machine_learning/ensemblemethods/ target=_blank>results of a Random Forests model</a> and with the <a href=/blog/machine_learning/support_vector_machine/ target=_blank>results of a Support Vector Machine</a>, the project finds Neural Networks that rank in between the two former methods described. Although all models are close together in their accuracy values, this ranking seems to be reproducible in a stable manner. This observation that Random Forests outperform even big Neural Networks might be a hint about the nature of deduplicating records. When deciding on whether a pair of records is a record of duplicates, a series of decisions on a sequence of attributes is taken. At each step, a decision is taken on whether the attributes are close to or far away from each other. A Decision Tree and eventually its statistically more sophisticated Random Forests implementation may be the most adequate algorithm for taking this series of decisions.</p><p>By the end of the project, a doubt has arisen whether this might be the only reason. Some suspicion on the reliability of the training data has appeared and human assessment of wrongly predicted records have generated the positive doubt that the prediction of the artificial machine may be better than the label of the training record. This is surprising and points out the requirement of looking closely at the process of generating the data and at the data itself. The scenario to be explored is to improve the quality of the training data by looking at the results of an artificial machine and hence learn from this machine about the data. Before doing so, a close look has to be taken at how the data is processed. After all methods are described, time is up to have a close look at the data, a look at how the data is processed, generated and at its final quality for training and performance assessment. This will be done in the next blog article.</p><p><a href=/blog/machine_learning/background_de>Einführungsartikel: warum beschäftigen wir uns mit Methoden des Maschinellen Lernens</a><br><a href=/blog/machine_learning/background_en>Introductory article: why do we deal with methods of machine learning</a><br><a href=/blog/machine_learning/ensemblemethods>Part1: ensemble methods</a><br><a href=/blog/machine_learning/support_vector_machine>Part2: support vector machines</a></p><p><a href=https://www.linkedin.com/in/andreas-jud-2a39a770/ target=_blank>Author: Andreas Jud</a></p><div class=tags><a href=/tags/neural-networks><svg width="20" class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path fill="currentcolor" d="M16 5l-.3125.28125L4.28125 16.8125l-.6875.6875.6875.71875 9.5 9.5.71875.6875.6875-.6875L26.71875 16.3125 27 16v-.40625V6 5H26 16.40625 16zm.84375 2H25v8.15625L14.5 25.59375 6.40625 17.5 16.84375 7zM22 9C21.447715 9 21 9.4477153 21 10 21 10.552285 21.447715 11 22 11S23 10.552285 23 10C23 9.4477153 22.552285 9 22 9z"/></svg>Neural networks </a><a href=/tags/english><svg width="20" class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path fill="currentcolor" d="M16 5l-.3125.28125L4.28125 16.8125l-.6875.6875.6875.71875 9.5 9.5.71875.6875.6875-.6875L26.71875 16.3125 27 16v-.40625V6 5H26 16.40625 16zm.84375 2H25v8.15625L14.5 25.59375 6.40625 17.5 16.84375 7zM22 9C21.447715 9 21 9.4477153 21 10 21 10.552285 21.447715 11 22 11S23 10.552285 23 10C23 9.4477153 22.552285 9 22 9z"/></svg>English </a><a href=/tags/2020><svg width="20" class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path fill="currentcolor" d="M16 5l-.3125.28125L4.28125 16.8125l-.6875.6875.6875.71875 9.5 9.5.71875.6875.6875-.6875L26.71875 16.3125 27 16v-.40625V6 5H26 16.40625 16zm.84375 2H25v8.15625L14.5 25.59375 6.40625 17.5 16.84375 7zM22 9C21.447715 9 21 9.4477153 21 10 21 10.552285 21.447715 11 22 11S23 10.552285 23 10C23 9.4477153 22.552285 9 22 9z"/></svg>2020th</a></div></article></section><footer><div class="items-2 items"><section><h2>Recent Blog Posts</h2><ul><li><a href=https://swissbib.github.io/blog/machine_learning/neural_networks/>An Attempt to Copy Human Learning with the Help of Artificial Neural Networks</a> -
<time datetime=2020-11-11T20:00:00+02:00>Wednesday, Nov 11, 2020.</time></li><li><a href=https://swissbib.github.io/blog/software_development/swissbib_containerized/>Containerized environment for swissbib discovery</a> -
<time datetime=2020-08-23T08:50:09+02:00>Sunday, Aug 23, 2020.</time></li><li><a href=https://swissbib.github.io/blog/machine_learning/support_vector_machine/>What Humans Learn about the Data from a Support Vector Machine</a> -
<time datetime=2020-08-20T16:07:50+02:00>Thursday, Aug 20, 2020.</time></li><li><a href=https://swissbib.github.io/blog/machine_learning/ensemblemethods/>A Machine Learning Approach with Ensemble Methods</a> -
<time datetime=2020-07-19T13:48:19+02:00>Sunday, Jul 19, 2020.</time></li><li><a href=https://swissbib.github.io/blog/machine_learning/background_de/>Warum maschinelles Lernen im swissbib Projekt</a> -
<time datetime=2020-07-18T15:50:09+02:00>Saturday, Jul 18, 2020.</time></li><li><a href=https://swissbib.github.io/blog/machine_learning/background_en/>Why machine learning in the swissbib project</a> -
<time datetime=2020-07-18T15:50:09+02:00>Saturday, Jul 18, 2020.</time></li></ul></section><section><h2>Contact Us</h2><ul class=contact-us><li><a target=_blank href=mailto:swissbib-ub@unibas.ch rel=noopener><svg width="30" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M3 8V9 25v1H4 28h1V25 9 8H28 4 3zm4.3125 2h17.375L16 15.78125 7.3125 10zM5 10.875l10.4375 6.96875L16 18.1875l.5625-.34375L27 10.875V24H5V10.875z"/></svg><span>swissbib-ub [at] unibas [dot] ch</span></a></li><li><a target=_blank href=https://twitter.com/swissbib rel=noopener><svg width="30" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M28 8.557c-.883.392-1.832.656-2.828.775 1.017-.609 1.797-1.574 2.165-2.724-.951.564-2.005.974-3.127 1.195-.898-.957-2.178-1.555-3.594-1.555-2.719.0-4.924 2.205-4.924 4.924.0.386.044.762.127 1.122-4.092-.205-7.72-2.166-10.149-5.145C5.247 7.876 5.004 8.722 5.004 9.625c0 1.708.869 3.215 2.19 4.098-.807-.026-1.566-.247-2.23-.616.0.021.0.041.0.062.0 2.386 1.697 4.376 3.95 4.828C8.501 18.11 8.066 18.17 7.616 18.17c-.317.0-.626-.031-.926-.088.627 1.956 2.445 3.38 4.6 3.42-1.685 1.321-3.808 2.108-6.115 2.108-.397.0-.789-.023-1.175-.069 2.179 1.397 4.767 2.212 7.548 2.212 9.057.0 14.009-7.503 14.009-14.01.0-.213-.005-.426-.014-.637C26.505 10.411 27.34 9.544 28 8.557z"/></svg><span>twitter.com/swissbib</span></a></li></ul></section></div><div id=copyright>Last Updated -
<time datetime=2020-11-12T08:00:29+01:00>Thursday, Nov 12, 2020.</time><br>Themed using <a href=https://github.com/hugoinaction/Eclectic target=_blank>Eclectic</a> by <a href=https://atishay.me target=_blank>Atishay</a></div></footer></body></html>