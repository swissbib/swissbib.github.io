<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=apple-mobile-web-app-capable content="yes"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#71B180"><meta name=generator content="Hugo 0.74.3"><meta name=robots content="index, follow"><title>Support_vector_machine | Acme Corporation</title><link rel=canonical href=https://swissbib.github.io/support_vector_machine/><meta name=description content="What Humans Can Learn about the Data from a Support Vector Machine What a Support Vector Machine Teaches Us about the Data Machine Learning may look mystical, although it is based on the mathematical discipline of statistics. If the learning process is done with a fixed data set, Machine Learning can be structured into two distinct groups of tasks, supervised and unsupervised learning. In supervised learning, the artificial machine is trained with a set of labelled training data."><meta name=author content="Acme Corporation"><link rel=manifest href=/manifest.json><meta property="og:title" content="Support_vector_machine"><meta property="og:site_name" content="Acme Corporation"><meta property="og:description" content="What Humans Can Learn about the Data from a Support Vector Machine What a Support Vector Machine Teaches Us about the Data Machine Learning may look mystical, although it is based on the mathematical discipline of statistics. If the learning process is done with a fixed data set, Machine Learning can be structured into two distinct groups of tasks, supervised and unsupervised learning. In supervised learning, the artificial machine is trained with a set of labelled training data."><meta property="og:type" content="article"><meta property="og:url" content="https://swissbib.github.io/support_vector_machine/"><meta property="article:published_time" content="2020-08-01T16:07:50+02:00"><meta property="article:modified_time" content="2020-08-01T16:07:50+02:00"><meta property="article:author" content="https://www.facebook.com/https://facebook.com/example"><meta property="article:publisher" content="https://www.facebook.com/https://facebook.com/example"><meta name=twitter:card content="summary"><meta name=twitter:title content="Support_vector_machine"><meta name=twitter:description content="What Humans Can Learn about the Data from a Support Vector Machine What a Support Vector Machine Teaches Us about the Data Machine Learning may look mystical, although it is based on the mathematical discipline of statistics. If the learning process is done with a fixed data set, Machine Learning can be structured into two distinct groups of tasks, supervised and unsupervised learning. In supervised learning, the artificial machine is trained with a set of labelled training data."><meta name=twitter:site content="@https://twitter.com/example"><meta name=twitter:creator content="@https://twitter.com/example"><script type=application/ld+json>{"headline":"Support_vector_machine","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/image/logo.min.svg"},"name":"Acme Corporation"},"author":{"@type":"Person","name":"Acme Corporation"},"description":"What Humans Can Learn about the Data from a Support Vector Machine What a Support Vector Machine Teaches Us about the Data Machine Learning may look mystical, although it is based on the mathematical discipline of statistics. If the learning process is done with a fixed data set, Machine Learning can be structured into two distinct groups of tasks, supervised and unsupervised learning. In supervised learning, the artificial machine is trained with a set of labelled training data.","name":"Acme Corporation","@type":"Article","wordCount":1213,"mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/swissbib.github.io\/support_vector_machine\/"},"@context":"http://schema.org","dateCreated":"2020-08-01T16:07:50+02:00","dateModified":"2020-08-01T16:07:50+02:00","url":"https://swissbib.github.io/support_vector_machine/"}</script><link rel=preload href=/index.json as=fetch crossorigin importance=low><link rel=stylesheet href=/css/index.min.3194927e75096c1cc15abc29992a1fe646736cb8e7c26bc10641a77c48cca41cfb179e59a2dc62e8c996a0969ae2a24a7aee43fa0c0b2331afc891736e4e3197.css media=none onload="if(media!='all')media='all'"><noscript><link rel=stylesheet href=/css/index.min.3194927e75096c1cc15abc29992a1fe646736cb8e7c26bc10641a77c48cca41cfb179e59a2dc62e8c996a0969ae2a24a7aee43fa0c0b2331afc891736e4e3197.css media=screen></noscript><style>html{opacity:0}</style><script type=text/javascript src=/index.min.js defer></script><script>if(window.localStorage.color){document.documentElement.style.setProperty("--theme-color",window.localStorage.color);}
if(window.localStorage.dark==="true"){document.documentElement.classList.add("dark");}</script></head><body><a href=#top class=scroll-up role=button><span class=sr-only>Scroll to top</span><svg width="30" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M16 6.59375l-.71875.6875-12.5 12.5 1.4375 1.4375L16 9.4375 27.78125 21.21875l1.4375-1.4375-12.5-12.5L16 6.59375z"/></svg></a><header id=header><input class=hamburger role=button type=checkbox aria-label="Menu Button">
<a class=icon aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 25 28"><g class="svg-menu-toggle"><path class="bar" d="M20.945 8.75c0 .69-.5 1.25-1.117 1.25H3.141c-.617.0-1.118-.56-1.118-1.25s.5-1.25 1.118-1.25h16.688C20.445 7.5 20.945 8.06 20.945 8.75z"/><path class="bar" d="M20.923 15c0 .689-.501 1.25-1.118 1.25H3.118C2.5 16.25 2 15.689 2 15s.5-1.25 1.118-1.25h16.687C20.422 13.75 20.923 14.311 20.923 15z"/><path class="bar" d="M20.969 21.25c0 .689-.5 1.25-1.117 1.25H3.164c-.617.0-1.118-.561-1.118-1.25s.5-1.25 1.118-1.25h16.688C20.469 20 20.969 20.561 20.969 21.25z"/></g></svg></a><a role=button class=logo href=https://swissbib.github.io/index.html><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 118 114.1" style="enable-background:new 0 0 118 114.1"><style>.st0{fill:#71b180}.st1{font-family:helveticaneue}.st2{font-size:28px}.st3{font-family:helveticaneue-thin}.st4{font-size:24px}</style><path class="st0" d="M104 15.8c-2.5 1.1-3.5-.1-4.7-.7-7.6-3.5-14.8-7.6-22.8-10.3-5-1.7-9.8-1.4-14.6.3-8.8 3.1-16.5 8.1-24.7 12.4-2.1 1.1-5.3 3.5-6.5 1.2s2.7-3.5 4.8-4.6c8-4.3 15.8-9.1 24.3-12.4C66.3-.7 72.6-.6 79 2 87.7 5.6 95.4 10.7 104 15.8z"/><path class="st0" d="M1.3 27.3c4.6-.4 9.3-.9 14-1 13.3-.2 26.7-.3 40-.3 1.5.0 3.8-.4 3.8 1.9.0 1.8-2 1.9-3.5 1.9C37.4 29.8 19.3 30.1 1.3 27.3z"/><text transform="matrix(1 0 0 1 22.9507 61.1348)" class="st1 st2">Acme</text><text transform="matrix(1 0 0 1 6.713867e-04 94.7348)" class="st3 st4">Corporation</text></svg><span class=sr-only>Logo</span></a><ul role=menubar class=top-menu><li role=menuitem class=top-menu-item><a href=/about>ABOUT</a></li><li role=menuitem class=top-menu-item><a href=/contact>CONTACT</a></li></ul><div id=searchbox><label class=sr-only for=search>Search</label>
<input id=search type=text placeholder=Search spellcheck=true><svg width="20" class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path fill="currentcolor" d="M19 3C13.488997 3 9 7.4889972 9 13 9 15.39499 9.8389508 17.588106 11.25 19.3125L3.28125 27.28125l1.4375 1.4375L12.6875 20.75C14.411894 22.161049 16.60501 23 19 23 24.511003 23 29 18.511003 29 13 29 7.4889972 24.511003 3 19 3zm0 2C23.430123 5 27 8.5698774 27 13 27 17.430123 23.430123 21 19 21 14.569877 21 11 17.430123 11 13 11 8.5698774 14.569877 5 19 5z"/></svg><section class=results></section></div><div id=scroll-indicator></div></header><section class="meta content with-background"><article><h1 class=title>Support_vector_machine</h1><h1 id=what-humans-can-learn-about-the-data-from-a-support-vector-machine>What Humans Can Learn about the Data from a Support Vector Machine</h1><h2 id=what-a-support-vector-machine-teaches-us-about-the-data>What a Support Vector Machine Teaches Us about the Data</h2><p>Machine Learning may look mystical, although it is based on the mathematical discipline of statistics. If the learning process is done with a fixed data set, Machine Learning can be structured into two distinct groups of tasks, supervised and unsupervised learning. In supervised learning, the artificial machine is trained with a set of labelled training data. In the first blog article, the feature records and their associated target value have been introduced. The target value on a feature record is the label, declaring wether the record is a record of a pair of duplicates or a record of a pair of uniques. Discarding the target value information from each feature record, the training data becomes unlabelled. In Machine Learning, there are also methods for training artificial machines on unlabelled data. These are called unsupervised learning. Support Vector Machines are a popular method, with its origins in the computer sciences community. Support Vector Classifiers are a method in the area of supervised learning. In this blog, the results of a Support Vector Classifier are presented together with a model of unsupervised learning. This gives some deeper insight into the underlying data.</p><p>In the first blog, the notion of a feature record has been introduced. From a mathematical point of view one single feature record can be considered a point in an n-dimensional space, with n equal to the number of twenty features in the swissbib project at hand. The deduplication problem of swissbib is a classification task. Explicitly, the artificial machine is trained to assign each point in the feature space to one of the two target classes, either the label pair of uniques or the label pair of duplicates. During the training process, a Support Vector Machine searches a boundary which separates the records of pairs of uniques from the records of pairs of duplicates. If this boundary is useful it clusters the points of the feature space that belong to the same class. In an n-dimensional space, this boundary is called a hyperplane. The best hyperplane for separating the records into its two classes may have a complicated mathematical representation. To avoid this issue and with the goal of separating the points with the help of a linear hyperplane, a Support Vector Classifier transforms the feature space, instead. This transformation from a linear space to a non-linear space is done with a kernel transformation of the machine. The kernel transformation ideally separates the data points of the two classes. This trick is called the kernel trick.</p><p>In the project described, a Support Vector Classifier is trained with the help of swissbib training data, using cross-validation. For the parameters of the classifier, a kernel with a polynomial transformation of third degree is found to generate the best results. With this classifier, an accuracy value of 99.88% can be reached. This accuracy is lower than the best accuracy values of the best Random Forests Classifier of the first blog article. It means explicitly 60 wrong predictions on a total of 51'886 validation records.</p><p>A Support Vector Classifier predicts the belonging of a data point to a class according to a probability value in the range of the interval from 0 to 1. The threshold value of 0.5 is the separator between the two classes. If the Support Vector Classifier calculates a probability value in the interval from 0 to 0.5, then the classifier model assigns the feature record to the class of pairs of uniques. If the classifier model calculates a probability value in the interval from 0.5 to 1, then the machine assigns the feature record to the class of pairs of duplicates. Figure 1 shows the distribution of the records for some part of swissbib training data. The green line in the figure represents the threshold of 0.5, separating the data into its two classes.</p><hr><h2 id=figure-1-probability-distribution-of-swissbib-training-data>es fehlt das Bild svc_prediction.png
Figure 1. Probability distribution of swissbib training data</h2><p>Looking at figure 1, it is very interesting to observe the distance of the points from the classes separating threshold. It looks like most of the pairs of uniques lie on a line with a probability value of 1, while many of the pairs of duplicates lie on a line with a probability value of 0. These extreme points with probabilities at the probability boundaries 0 and 1 are the easy cases. More interesting are the points that have probablity values between 1 and 0. These are the cases expressing some uncertainty. Even most interesting are the cases which are located on the wrong side of their class. These are the wrong predictions of the Support Vector Classifier. Figure 1 shows some pairs of uniques in the domain area of duplicates and some pairs of duplicates in the domain area of uniques. These points have to be investigated deeper to understand why they are wrongly assigned. This task has to be done manually and will be subject of a later blog to come.</p><p>Despite of the wrong predictions, figure 1 proves that the Support Vector Classifier trained is a useful model for predicting the class on swissbib data. There seems to be some criteria in the data that separate the two classes. Machine Learning offers several methods of unsupervised learning to investigate and eventually confirm this assumption. As an example, the capstone project uses the method of k-means for further analysing the data. This method tries to cluster unlabelled data generating a number of k clusters. For the swissbib project at hand, k=2 is suggested, namely for one class of records of pairs of uniques and the other class of records of pairs of duplicates. After training a k-means model with k=2 on the unlabelled training data, the predicted points can be relabelled as the target values for the training data is known. Assigning the color of red to records of pairs of duplicates and the color of blue to records of uniques to the predictions of the trained k-means model, generates the graph of figure 2. This figure shows the red points well clustered in a separated way from the cluster of blue points. This confirms that the data has some properties that help separating pairs of uniques from pairs of duplicates.</p><hr><h2 id=es-fehlt-figure-2-k-means-clustering-on-training-data-k-means_clusterspng>es fehlt: Figure 2. k-means clustering on training data (k-means_clusters.png)</h2><p>This blog article offers some insight into the properties of swissbib training data. This insight can be deepened analysing the underlying models and its statements. In this manner, the trained machines talk to humans explaining their mathematical background in a compacted manner. Discussing these results of different aspects helps humans to improve their understanding of the data. Humans can learn from the models, they can learn from the trained machines. This aspect of interaction between the two entities humans and machines is an important aspect, hardly discussed in the debate on Machine Learning and artificial intelligence.</p><p>When talking about intelligence, the human brain is considered to be its domicile. The human brain is an incredibly powerful network of neurons and therefore, Machine Learning tries to find results with the help of networks that simulate the human brain. The next blog article will explain the results of a Neural Network.</p></article></section><footer><div class="items-3 items"><section><h2>About</h2><p>Acme Corporation is the world&rsquo;s leading manufacturer of digital shapes. From squares and circles to triangles and hexagons, we have it all. Browse through our collection of various forms with different thickness and line styles. We shape the world. You live in it.</p></section><section><h2>Recent Blog Posts</h2><ul></ul></section><section><h2>Contact Us</h2><ul class=contact-us><li><a target=_blank href=mailto:contact@example.org rel=noopener><svg width="30" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M3 8V9 25v1H4 28h1V25 9 8H28 4 3zm4.3125 2h17.375L16 15.78125 7.3125 10zM5 10.875l10.4375 6.96875L16 18.1875l.5625-.34375L27 10.875V24H5V10.875z"/></svg><span>contact [at] example [dot] org</span></a></li><li><a target=_blank href=https://twitter.com/example rel=noopener><svg width="30" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M28 8.557c-.883.392-1.832.656-2.828.775 1.017-.609 1.797-1.574 2.165-2.724-.951.564-2.005.974-3.127 1.195-.898-.957-2.178-1.555-3.594-1.555-2.719.0-4.924 2.205-4.924 4.924.0.386.044.762.127 1.122-4.092-.205-7.72-2.166-10.149-5.145C5.247 7.876 5.004 8.722 5.004 9.625c0 1.708.869 3.215 2.19 4.098-.807-.026-1.566-.247-2.23-.616.0.021.0.041.0.062.0 2.386 1.697 4.376 3.95 4.828C8.501 18.11 8.066 18.17 7.616 18.17c-.317.0-.626-.031-.926-.088.627 1.956 2.445 3.38 4.6 3.42-1.685 1.321-3.808 2.108-6.115 2.108-.397.0-.789-.023-1.175-.069 2.179 1.397 4.767 2.212 7.548 2.212 9.057.0 14.009-7.503 14.009-14.01.0-.213-.005-.426-.014-.637C26.505 10.411 27.34 9.544 28 8.557z"/></svg><span>twitter.com/example</span></a></li><li><a target=_blank href=https://facebook.com/example rel=noopener><svg width="30" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M7 5C5.9069372 5 5 5.9069372 5 7V25C5 26.093063 5.9069372 27 7 27H25C26.093063 27 27 26.093063 27 25V7C27 5.9069372 26.093063 5 25 5H7zM7 7H25V25H19.8125V18.25h2.59375l.375-3H19.8125V13.3125C19.8125 12.4385 20.02825 11.84375 21.28125 11.84375h1.625V9.125C22.62925 9.088 21.6665 9.03125 20.5625 9.03125 18.2585 9.03125 16.6875 10.417 16.6875 13v2.25h-2.625v3h2.625V25H7V7z"/></svg><span>facebook.com/example</span></a></li></ul></section></div><div id=copyright>Copyright Â© 2020 Acme Corporation. All Rights Reserved. Last Updated -
<time datetime=2020-08-12T10:05:20+02:00>Wednesday, Aug 12, 2020.</time><br>Themed using <a href>Eclectic</a> by <a href=https://atishay.me>Atishay</a></div></footer></body></html>