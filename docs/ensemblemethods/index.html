<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=apple-mobile-web-app-capable content="yes"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#71B180"><meta name=generator content="Hugo 0.74.3"><meta name=robots content="index, follow"><title>Ensemblemethods | Acme Corporation</title><link rel=canonical href=https://swissbib.github.io/ensemblemethods/><meta name=description content="The notion of Machine Learning includes a wide group of statistical algorithms where a computer system learns on a set of training data and, after having completed its learning phase, uses its experience to generate predictions on new, unknown data. In the context of the capstone project of an advanced online training course at EPF Lausanne, a swissbib admirer takes the challenge to do Machine Learning with a set of library catalogue data in MARC 21 format."><meta name=author content="Acme Corporation"><link rel=manifest href=/manifest.json><meta property="og:title" content="Ensemblemethods"><meta property="og:site_name" content="Acme Corporation"><meta property="og:description" content="The notion of Machine Learning includes a wide group of statistical algorithms where a computer system learns on a set of training data and, after having completed its learning phase, uses its experience to generate predictions on new, unknown data. In the context of the capstone project of an advanced online training course at EPF Lausanne, a swissbib admirer takes the challenge to do Machine Learning with a set of library catalogue data in MARC 21 format."><meta property="og:type" content="article"><meta property="og:url" content="https://swissbib.github.io/ensemblemethods/"><meta property="article:published_time" content="2020-08-01T13:48:19+02:00"><meta property="article:modified_time" content="2020-08-01T13:48:19+02:00"><meta property="article:author" content="https://www.facebook.com/https://facebook.com/example"><meta property="article:publisher" content="https://www.facebook.com/https://facebook.com/example"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ensemblemethods"><meta name=twitter:description content="The notion of Machine Learning includes a wide group of statistical algorithms where a computer system learns on a set of training data and, after having completed its learning phase, uses its experience to generate predictions on new, unknown data. In the context of the capstone project of an advanced online training course at EPF Lausanne, a swissbib admirer takes the challenge to do Machine Learning with a set of library catalogue data in MARC 21 format."><meta name=twitter:site content="@https://twitter.com/example"><meta name=twitter:creator content="@https://twitter.com/example"><script type=application/ld+json>{"headline":"Ensemblemethods","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/image/logo.min.svg"},"name":"Acme Corporation"},"author":{"@type":"Person","name":"Acme Corporation"},"description":"The notion of Machine Learning includes a wide group of statistical algorithms where a computer system learns on a set of training data and, after having completed its learning phase, uses its experience to generate predictions on new, unknown data. In the context of the capstone project of an advanced online training course at EPF Lausanne, a swissbib admirer takes the challenge to do Machine Learning with a set of library catalogue data in MARC 21 format.","name":"Acme Corporation","@type":"Article","wordCount":1300,"mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/swissbib.github.io\/ensemblemethods\/"},"@context":"http://schema.org","dateCreated":"2020-08-01T13:48:19+02:00","dateModified":"2020-08-01T13:48:19+02:00","url":"https://swissbib.github.io/ensemblemethods/"}</script><link rel=preload href=/index.json as=fetch crossorigin importance=low><link rel=stylesheet href=/css/index.min.3194927e75096c1cc15abc29992a1fe646736cb8e7c26bc10641a77c48cca41cfb179e59a2dc62e8c996a0969ae2a24a7aee43fa0c0b2331afc891736e4e3197.css media=none onload="if(media!='all')media='all'"><noscript><link rel=stylesheet href=/css/index.min.3194927e75096c1cc15abc29992a1fe646736cb8e7c26bc10641a77c48cca41cfb179e59a2dc62e8c996a0969ae2a24a7aee43fa0c0b2331afc891736e4e3197.css media=screen></noscript><style>html{opacity:0}</style><script type=text/javascript src=/index.min.js defer></script><script>if(window.localStorage.color){document.documentElement.style.setProperty("--theme-color",window.localStorage.color);}
if(window.localStorage.dark==="true"){document.documentElement.classList.add("dark");}</script></head><body><a href=#top class=scroll-up role=button><span class=sr-only>Scroll to top</span><svg width="30" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M16 6.59375l-.71875.6875-12.5 12.5 1.4375 1.4375L16 9.4375 27.78125 21.21875l1.4375-1.4375-12.5-12.5L16 6.59375z"/></svg></a><header id=header><input class=hamburger role=button type=checkbox aria-label="Menu Button">
<a class=icon aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 25 28"><g class="svg-menu-toggle"><path class="bar" d="M20.945 8.75c0 .69-.5 1.25-1.117 1.25H3.141c-.617.0-1.118-.56-1.118-1.25s.5-1.25 1.118-1.25h16.688C20.445 7.5 20.945 8.06 20.945 8.75z"/><path class="bar" d="M20.923 15c0 .689-.501 1.25-1.118 1.25H3.118C2.5 16.25 2 15.689 2 15s.5-1.25 1.118-1.25h16.687C20.422 13.75 20.923 14.311 20.923 15z"/><path class="bar" d="M20.969 21.25c0 .689-.5 1.25-1.117 1.25H3.164c-.617.0-1.118-.561-1.118-1.25s.5-1.25 1.118-1.25h16.688C20.469 20 20.969 20.561 20.969 21.25z"/></g></svg></a><a role=button class=logo href=https://swissbib.github.io/index.html><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 118 114.1" style="enable-background:new 0 0 118 114.1"><style>.st0{fill:#71b180}.st1{font-family:helveticaneue}.st2{font-size:28px}.st3{font-family:helveticaneue-thin}.st4{font-size:24px}</style><path class="st0" d="M104 15.8c-2.5 1.1-3.5-.1-4.7-.7-7.6-3.5-14.8-7.6-22.8-10.3-5-1.7-9.8-1.4-14.6.3-8.8 3.1-16.5 8.1-24.7 12.4-2.1 1.1-5.3 3.5-6.5 1.2s2.7-3.5 4.8-4.6c8-4.3 15.8-9.1 24.3-12.4C66.3-.7 72.6-.6 79 2 87.7 5.6 95.4 10.7 104 15.8z"/><path class="st0" d="M1.3 27.3c4.6-.4 9.3-.9 14-1 13.3-.2 26.7-.3 40-.3 1.5.0 3.8-.4 3.8 1.9.0 1.8-2 1.9-3.5 1.9C37.4 29.8 19.3 30.1 1.3 27.3z"/><text transform="matrix(1 0 0 1 22.9507 61.1348)" class="st1 st2">Acme</text><text transform="matrix(1 0 0 1 6.713867e-04 94.7348)" class="st3 st4">Corporation</text></svg><span class=sr-only>Logo</span></a><ul role=menubar class=top-menu><li role=menuitem class=top-menu-item><a href=/about>ABOUT</a></li><li role=menuitem class=top-menu-item><a href=/contact>CONTACT</a></li></ul><div id=searchbox><label class=sr-only for=search>Search</label>
<input id=search type=text placeholder=Search spellcheck=true><svg width="20" class="icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path fill="currentcolor" d="M19 3C13.488997 3 9 7.4889972 9 13 9 15.39499 9.8389508 17.588106 11.25 19.3125L3.28125 27.28125l1.4375 1.4375L12.6875 20.75C14.411894 22.161049 16.60501 23 19 23 24.511003 23 29 18.511003 29 13 29 7.4889972 24.511003 3 19 3zm0 2C23.430123 5 27 8.5698774 27 13 27 17.430123 23.430123 21 19 21 14.569877 21 11 17.430123 11 13 11 8.5698774 14.569877 5 19 5z"/></svg><section class=results></section></div><div id=scroll-indicator></div></header><section class="meta content with-background"><article><h1 class=title>Ensemblemethods</h1><p>The notion of Machine Learning includes a wide group of statistical algorithms where a computer system learns on a set of training data and, after having completed its learning phase, uses its experience to generate predictions on new, unknown data. In the context of the capstone project of an advanced online training course at EPF Lausanne, a swissbib admirer takes the challenge to do Machine Learning with a set of library catalogue data in MARC 21 format. The goal of the project is to build an artificial machine being capable to find duplicate records in the data. The project is done with three distinct groups of models. In this blog, the results of a Decision Tree and a Random Forests model are presented.</p><p>The starting point for Machine Learning is data. The data generally consists of two distinct types of variables, features and its target. The variables of the data set that serve as input for computing the prediction are called features. The features of the data are constructed with the help of two records of original swissbib data containing raw information of a bibliographic unit, each. Such two records of raw data are paired in each of its attributes, calculating a numerical similarity distance for each pair of same attributes of the two bibliographical records, see figure 1. For example, for two arbitrary records, the mathematical distance between title1 (title of record r1) and title2 (title of record r2) are determined to be the feature titleΔ = sim(title1, title2), where sim(x1, x2) is a mathematical similarity function. For the project at hand, twenty distinct similarities of two times twenty raw data attributes like title, author, year, ISBN, ISMN, etc. are calculated for each feature record. Therefore, the number of features are twenty. A feature record is represented as an array in the memory of a computer system. All rows of feature arrays can be represented in the form of a matrix. Therefore, the full set of this data is called feature matrix. The total of feature records used for training the data is nearly 260'000.</p><h2 id=a-hrefhttpswwwswissbibch-target_blankimg-style-width-800px-height-500px-srcimagerecord_pairingpnga><a href=https://www.swissbib.ch target=_blank><img style=width:800px;height:500px src=/image/record_pairing.png></a></h2><h2 id=figure-1-records-pairing>figure 1, records pairing</h2><p>The variable of a data set that is to be predicted by the machine is called output or target, see figure 2. Each feature record has its target value. For each feature record of the project described above, the target variable indicates whether a data row of features is either a row of unique records or a row of duplicate records. The possible target values are 0 (row of unique records) or 1 (row of duplicate records), resp.. A more detailed description on how to calculate the feature matrix and its array of target values for the training data will be given in a later blog to come.</p><p><a href=https://www.swissbib.ch target=_blank><img style=width:800px;height:500px src=/image/features_model_target.png></a></p><hr><h2 id=figure-2-feature-matrix>figure 2, feature matrix</h2><p>The idea behind a Decision Tree algorithm is that the computer system learns a set of sequential if-then-else rules that lead to a final decision. Each if-then-else statement is called a node of the Decision Tree. The nodes are arranged in sequences to form of a binary tree, see figure 3, which is the reason for its naming. In the swissbib project, the set of if-then-else rules is a sequence of thresholds for binary statements of one feature variable that can either be lower or higher than the specific threshold. To classify a feature record, the algorithm starts at the top of the tree and evaluates he statement in each node on its path down. Depending on the threshold value, the algorithm decides for the right-lower or the left-lower node as the next node until reaching the bottom of the tree. The final decision is called a leaf of the Decision Tree. The leaf concludes the decision wether the feature record is a pair of uniques or a pair of duplicates. During training a Decision Tree, the if-then-else rules of the nodes are adjusted iteratively until the Decision Tree predicts the target value of the training data feature rows with the highest possible probability according to a function to measure the quality of decisions, called criterion. When this highest power of prediction on the training data is reached, the Decision Tree can be used for predicting new, unseen data.</p><h2 id=a-hrefhttpswwwswissbibch-target_blankimg-style-width-800px-height-500px-srcimagedecision_tree_cv_manualpnga><a href=https://www.swissbib.ch target=_blank><img style=width:800px;height:500px src=/image/decision_tree_cv_manual.png></a></h2><h2 id=figure-3-graphical-representation-of-decision-tree>figure 3, graphical representation of decision tree</h2><p>Decision Tree is a classical method of Machine Learning. Its advantage is its clarity. It can be easily interpreted when looking at the trained model tree. A Decision Tree classifier can be built with the help of different parameters. In the project, the varied parameters are the maximum depth of the tree as well as the so called criterion, the mathematical function to measure the quality of a split in the nodes. Several specific Decision Trees are calculated with the help of cross-validation and their prediction power is compared. The project finds the best Decision Tree for swissbib data to have a maximum depth of 26 nodes and a criterion of Gini impurity.</p><p>The performance of a Machine Learning classifier can be measured with the help of some metrics derived from the confusion matrix, see figure 4. The confusion matrix compares the predictions of a trained machine on some validation data with their given target values. Four cases can be distinguished.</p><ul><li>Two &ldquo;true&rdquo; cases (1. &ldquo;true positive&rdquo; and 2. &ldquo;true negative&rdquo;) according to the two specific classes are the correctly predicted records of the validation data.</li><li>Two &ldquo;false&rdquo; cases (3. &ldquo;false positive&rdquo; and 4. &ldquo;false negative&rdquo;) according to the two specific classes are the wrongly predicted records of the validation data.</li></ul><h2 id=a-hrefhttpswwwswissbibch-target_blankimg-style-width-800px-height-500px-srcimage04_confusion_matrixpnga><a href=https://www.swissbib.ch target=_blank><img style=width:800px;height:500px src=/image/04_confusion_matrix.png></a></h2><h2 id=figure-4-confusion-matrix>figure 4, confusion matrix</h2><p>From the four cases above, a metrics called accuracy can be calculated, allowing a statement on the prediction quality of the model on unknown data. For swissbib&rsquo;s calculated Decision Tree, an accuracy value of nearly 99.95% can be reached. This accuracy means 27 wrongly predicted records on a total of 51'886 validation records.</p><p>For comparison reasons, a Random Forests model is calculated additionally. A Random Forests is an Ensemble method. It consists of an ensemble of Decision Trees that are assembled during the<br>learning phase. Again, the set of best parameters for the Random Forests is searched for swissbib data and a number of 100 trees of maximum depth of 22 each in the forest is found to generate the best results. With Random Forests, an accuracy of nearly 99.95% can be reached, too, with the same<br>total of wrongly predicted records of 27 on the total of the validation records.</p><p>The project is implemented in programming language Python, using library scikit-learn for calculating the models. The Random Forests implementation of this library allows for assessing the importance of each feature for prediction. Figure 5 shows the normed importance value of the features used. It can be seen that variable year is the leading variable for indicating wether a pair of records is a pair of uniques or of duplicates. Variable title is the second most important feature for the Random Forests model, but also author and volumes indication are of high relevance. The importance of features like coordinate and ISMN seem to be low. This is due to the fact that only few of swissbib&rsquo;s raw data are of format map of music. Therefore, only few of swissbib&rsquo;s raw data hold any information in these attributes.</p><h2 id=a-hrefhttpswwwswissbibch-target_blankimg-style-width-800px-height-500px-srcimagefeature_importance_hrpnga><a href=https://www.swissbib.ch target=_blank><img style=width:800px;height:500px src=/image/feature_importance_hr.png></a></h2><h2 id=figure-5-normed-features-random-forest>figure 5, normed features, random forest</h2><p>The results presented here, suggest swissbib to implement a new deduplication process with the help of a Random Forests algorithm, due its best overall performance on the training data. The project described here, implements some more models different to the Decision Tree and the Random Forests models. The results of those will be presented in some additional blog articles.</p><p><a href=/background_de>Einführungsartikel: warum beschäftigen wir uns mit Mehdoden des Maschinellen Lernens</a><br><a href=/background_en>Introductory article: why do we deal with methods of machine learning</a><br><a href=/support_vector_machine>Part2: support vector machines</a></p></article></section><footer><div class="items-3 items"><section><h2>About</h2><p>Acme Corporation is the world&rsquo;s leading manufacturer of digital shapes. From squares and circles to triangles and hexagons, we have it all. Browse through our collection of various forms with different thickness and line styles. We shape the world. You live in it.</p></section><section><h2>Recent Blog Posts</h2><ul></ul></section><section><h2>Contact Us</h2><ul class=contact-us><li><a target=_blank href=mailto:contact@example.org rel=noopener><svg width="30" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M3 8V9 25v1H4 28h1V25 9 8H28 4 3zm4.3125 2h17.375L16 15.78125 7.3125 10zM5 10.875l10.4375 6.96875L16 18.1875l.5625-.34375L27 10.875V24H5V10.875z"/></svg><span>contact [at] example [dot] org</span></a></li><li><a target=_blank href=https://twitter.com/example rel=noopener><svg width="30" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M28 8.557c-.883.392-1.832.656-2.828.775 1.017-.609 1.797-1.574 2.165-2.724-.951.564-2.005.974-3.127 1.195-.898-.957-2.178-1.555-3.594-1.555-2.719.0-4.924 2.205-4.924 4.924.0.386.044.762.127 1.122-4.092-.205-7.72-2.166-10.149-5.145C5.247 7.876 5.004 8.722 5.004 9.625c0 1.708.869 3.215 2.19 4.098-.807-.026-1.566-.247-2.23-.616.0.021.0.041.0.062.0 2.386 1.697 4.376 3.95 4.828C8.501 18.11 8.066 18.17 7.616 18.17c-.317.0-.626-.031-.926-.088.627 1.956 2.445 3.38 4.6 3.42-1.685 1.321-3.808 2.108-6.115 2.108-.397.0-.789-.023-1.175-.069 2.179 1.397 4.767 2.212 7.548 2.212 9.057.0 14.009-7.503 14.009-14.01.0-.213-.005-.426-.014-.637C26.505 10.411 27.34 9.544 28 8.557z"/></svg><span>twitter.com/example</span></a></li><li><a target=_blank href=https://facebook.com/example rel=noopener><svg width="30" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M7 5C5.9069372 5 5 5.9069372 5 7V25C5 26.093063 5.9069372 27 7 27H25C26.093063 27 27 26.093063 27 25V7C27 5.9069372 26.093063 5 25 5H7zM7 7H25V25H19.8125V18.25h2.59375l.375-3H19.8125V13.3125C19.8125 12.4385 20.02825 11.84375 21.28125 11.84375h1.625V9.125C22.62925 9.088 21.6665 9.03125 20.5625 9.03125 18.2585 9.03125 16.6875 10.417 16.6875 13v2.25h-2.625v3h2.625V25H7V7z"/></svg><span>facebook.com/example</span></a></li></ul></section></div><div id=copyright>Copyright © 2020 Acme Corporation. All Rights Reserved. Last Updated -
<time datetime=2020-08-12T10:05:20+02:00>Wednesday, Aug 12, 2020.</time><br>Themed using <a href>Eclectic</a> by <a href=https://atishay.me>Atishay</a></div></footer></body></html>